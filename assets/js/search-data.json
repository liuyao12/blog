{
  
    
        "post0": {
            "title": "Title",
            "content": "Introducing Convolutional Layer with a Twist . I&#39;m excited to introduce conv_twist, a replacement of (and an improvement on) the good old convolutional layer widely used in Deep Learning models (rightfully referred to as ConvNets or CNN) in Computer Vision. Famously introduced by Yann LeCun some 30 years ago into image classification, it became the source of the Deep Learning/Artificial Intelligence revolution with AlexNet in 2012. Rapid improvements on the architecture followed, most notably the ResNet of 2015. Recently attention has somewhat shifted away from image classification, but convolutional layers are still the bread and butter of any Computer Vision models. What more can be said about convolutional layers, one might ask? The answer comes from a little bit of mathematics. . Long story short, here is one implementation of conv_twist in PyTorch, and you can easily swap out the 3x3 Conv2d in your model and plug this in, and train from scratch to see if it gives any improvement. (Report on results are welcome.) . import torch import torch.nn as nn import torchvision class conv_twist(nn.Module): # replacing 3x3 Conv2d def __init__(self, ni, nf, init_max=1.5, stride=1): super(conv_twist, self).__init__() self.conv = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False) self.conv_x = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False) self.conv_y = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False) self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2 # make conv_x a &quot;first-order operator&quot; by symmetrizing it self.conv_y.weight.data = self.conv_x.weight.transpose(2,3).flip(3) # make conv_y a 90 degree rotation of convx self.center_x = nn.Parameter(torch.Tensor(nf), requires_grad=True) self.center_y = nn.Parameter(torch.Tensor(nf), requires_grad=True) self.center_x.data.uniform_(-init_max, init_max) self.center_y.data.uniform_(-init_max, init_max) def forward(self, x): self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2 self.conv_y.weight.data = (self.conv_y.weight - self.conv_y.weight.flip(2).flip(3)) / 2 x1 = self.conv(x) _, c, h, w = x1.size() XX = torch.from_numpy(np.indices((1,h,w))[2]*2/w).type(x.dtype).to(x.device) - self.center_x.view(-1,1,1) YY = torch.from_numpy(np.indices((1,h,w))[1]*2/h).type(x.dtype).to(x.device) - self.center_y.view(-1,1,1) return x1 + (XX * self.conv_x(x) + YY * self.conv_y(x)) . Let&#39;s take a look at the weights in such a conv_twist model: . model = conv_twist(1,1) for name, para in model.named_parameters(): print(name, para) . center_x Parameter containing: tensor([0.2157], requires_grad=True) center_y Parameter containing: tensor([0.9144], requires_grad=True) conv.weight Parameter containing: tensor([[[[ 0.0164, 0.0767, -0.2709], [-0.3086, -0.0439, 0.1440], [-0.2906, 0.0374, 0.2853]]]], requires_grad=True) conv_x.weight Parameter containing: tensor([[[[ 0.2468, -0.2211, -0.2301], [-0.0661, 0.0000, 0.0661], [ 0.2301, 0.2211, -0.2468]]]], requires_grad=True) conv_y.weight Parameter containing: tensor([[[[ 0.2301, -0.0661, 0.2468], [ 0.2211, 0.0000, -0.2211], [-0.2468, 0.0661, -0.2301]]]], requires_grad=True) . If you take a look at the conv_x weights, you&#39;d notice that each 3x3 kernel is symmetric (the numbers on the opposite ends are identical except for the signs, with the middle one always 0). That&#39;s the effect of &quot;symmetrizing&quot; on conv_x, done at each forward pass. You can also check that the conv_x and conv_y weights are initialized to be identical but off by a 90 degree rotation. . Why do I choose to initialize the weights this way? Well, I&#39;ll try to explain all this later. For now it&#39;s important to note that conv_twist is a lot bigger than the standard Conv2d layer, but not as much as it appears to be. This particular implementation, if I had done it properly, has about twice as many trainable weights as a single Conv2d layer. . What are convolutions? . The classic storyline in Neural Networks is that the convolution operator captures the spatial relation of the pixels, so is particularly suited for image-related learning task, and has much fewer weights than a generic linear map (fully connected layer). And over the years people learned that we don&#39;t need kernels that are larger than 3x3, and to go deeper (i.e., many layers) instead, hence deep learning. . What is perhaps not well-known is that different 3x3 kernels have rather intuitive meanings, in terms of what it does to the image overall. For example, the Gaussian kernel in image processing &quot;blurs&quot; the image. We can do a little experiment to see: . G = torch.Tensor([[1,2,1],[2,4,2],[1,2,1]])/16 conv = nn.Conv2d(1,1,kernel_size=3, bias=False) conv.weight.data = G for name, param in conv.named_parameters(): print(name, param) # take a grayscale image, and feed it into the conv model. Display the result as an image side-by-side the original. . weight Parameter containing: tensor([[0.0625, 0.1250, 0.0625], [0.1250, 0.2500, 0.1250], [0.0625, 0.1250, 0.0625]], requires_grad=True) . To illustrate the effect for other 3x3 kernels, it seems best to choose the kernel close to the &quot;identity&quot;, and to apply it many times. Try the following: . A = torch.Tensor([[-1,0,1],[-2,0,2],[-1,0,1]]) B = torch.Tensor([[1,2,1],[0,0,0],[-1,-2,-1]]) I = torch.Tensor([[0,0,0],[0,1,0],[0,0,0]]) K = I + 0.01 * A . What does conv_twist do? . In addition to a normal Conv2d layer, the conv_twist is feeding the input to two other 3x3 Conv2d layer. To see the effect, let&#39;s turn off the conv weights . model.conv.data = 0 model.center_x.data = torch.Tensor([0]) model.center_y.data = torch.Tensor([0]) model.conv_x.data = I + 0.01 * A model.conv_y.data = I + 0.01 * B .",
            "url": "https://liuyao12.github.io/blog/2020/03/02/Convolutional-Layer-with-a-Twist.html",
            "relUrl": "/2020/03/02/Convolutional-Layer-with-a-Twist.html",
            "date": " • Mar 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://liuyao12.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://liuyao12.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://liuyao12.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}