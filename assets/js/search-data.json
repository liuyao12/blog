{
  
    
        "post0": {
            "title": "",
            "content": "I&#39;m excited to introduce conv_twist, a replacement of — and, I argue, an improvement on — the good old convolutional layer widely used in Deep Learning models (rightfully referred to as ConvNets or CNNs) in Computer Vision. Famously introduced by Yann LeCun some 30 years ago into image classification, it became the source of the Deep Learning/Artificial Intelligence revolution with AlexNet in 2012. Rapid improvements on the architecture followed, most notably the ResNet of 2015. Recently attention has somewhat shifted away from image classification, but convolutional layers are still the bread and butter of any Computer Vision models. What more can be said about convolutional layers, one might ask? The answer is a little bit of mathematics. . For what it&#39;s worth, this is not peer-reviewed or published in any conference. If you&#39;d like to give it a &quot;review&quot; on Twitter, feel free to do so. . PyTorch implementation . Without further ado, here is one (crude) implementation of conv_twist in PyTorch, and you can easily swap out the 3x3 Conv2d in your model and plug this in, and train from scratch to see if it gives any improvement. Early success has been shown on Imagenette/Imagewoof benchmark. (Help with testing on other models/datasets are greatly appreciated.) . import torch import torch.nn as nn import torchvision class conv_twist(nn.Module): # replacing 3x3 Conv2d def __init__(self, ni, nf, init_max=1.5, stride=1): super(conv_twist, self).__init__() self.conv = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False) self.conv_x = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False) self.conv_y = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False) self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2 # make conv_x a &quot;first-order operator&quot; by symmetrizing it self.conv_y.weight.data = self.conv_x.weight.transpose(2,3).flip(3) # make conv_y a 90 degree rotation of conv_x self.center_x = nn.Parameter(torch.Tensor(nf), requires_grad=True) self.center_y = nn.Parameter(torch.Tensor(nf), requires_grad=True) self.center_x.data.uniform_(-init_max, init_max) self.center_y.data.uniform_(-init_max, init_max) def forward(self, inpt): self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2 self.conv_y.weight.data = (self.conv_y.weight - self.conv_y.weight.flip(2).flip(3)) / 2 out = self.conv(inpt) _, c, h, w = out.size() XX = torch.from_numpy(np.indices((1,h,w))[2]*2/w).type(out.dtype).to(out.device) - self.center_x.view(-1,1,1) YY = torch.from_numpy(np.indices((1,h,w))[1]*2/h).type(out.dtype).to(out.device) - self.center_y.view(-1,1,1) return out + (XX * self.conv_x(inpt) + YY * self.conv_y(inpt)) . Let&#39;s take a look at the (initial) weights in such a conv_twist model: . model = conv_twist(1,3) for name, para in model.named_parameters(): print(name, para) . center_x Parameter containing: tensor([ 0.6307, -1.4049, -0.1047], requires_grad=True) center_y Parameter containing: tensor([-0.1879, 0.6662, 1.1414], requires_grad=True) conv.weight Parameter containing: tensor([[[[-0.2893, -0.0233, -0.2600], [ 0.2398, -0.1335, 0.0162], [ 0.1423, -0.0215, 0.1642]]], [[[-0.1147, -0.0316, -0.0759], [ 0.2738, -0.2315, 0.1012], [ 0.0793, 0.3118, 0.0178]]], [[[ 0.1192, 0.2551, 0.3169], [-0.0515, 0.1144, -0.0921], [-0.1128, -0.1987, -0.0691]]]], requires_grad=True) conv_x.weight Parameter containing: tensor([[[[-0.0199, -0.2608, -0.1219], [-0.0325, 0.0000, 0.0325], [ 0.1219, 0.2608, 0.0199]]], [[[ 0.2378, 0.0571, -0.1488], [-0.1910, 0.0000, 0.1910], [ 0.1488, -0.0571, -0.2378]]], [[[-0.1464, -0.0639, 0.0475], [ 0.1355, 0.0000, -0.1355], [-0.0475, 0.0639, 0.1464]]]], requires_grad=True) conv_y.weight Parameter containing: tensor([[[[ 0.1219, -0.0325, -0.0199], [ 0.2608, 0.0000, -0.2608], [ 0.0199, 0.0325, -0.1219]]], [[[ 0.1488, -0.1910, 0.2378], [-0.0571, 0.0000, 0.0571], [-0.2378, 0.1910, -0.1488]]], [[[-0.0475, 0.1355, -0.1464], [ 0.0639, 0.0000, -0.0639], [ 0.1464, -0.1355, 0.0475]]]], requires_grad=True) . If you take a look at the conv_x and conv_y weights, you&#39;d notice that each 3x3 kernel is symmetric (the numbers on the opposite ends of the square are identical but with opposite signs, with the middle one always 0). That&#39;s the effect of &quot;symmetrizing&quot;, done at each forward pass. You can also check that conv_y weights are initialized to be a 90 degree rotation of conv_x. . Why do I choose to initialize the weights this way, and what are XX and YY? Well, I&#39;ll try to explain everything later. For now it&#39;s important to note that conv_twist is a lot bigger than the standard Conv2d layer, but not as much as it appears to be. This particular implementation, if I had done it properly, has about twice as many trainable weights as a classic Conv2d layer. . What do convolutions actually do? . The classic storyline in Neural Networks is that the convolution operator captures the spatial relation of the pixels (local features), so is particularly suited for image-related learning task; and secondly it has much fewer weights than a generic linear map (fully connected layer). Over the years, people learned that it&#39;s better to do away with kernels that are larger than 3x3 (and any biases), and to go deeper (i.e., many layers) instead, hence deep learning. . What is perhaps not well-known is that different 3x3 kernels have rather intuitive meanings, in terms of what it does to the image overall. For example, the Gaussian kernel in image processing &quot;blurs&quot; the image. We can do a little experiment to see: . G = torch.Tensor([[1,2,1],[2,4,2],[1,2,1]]) / 16 conv = nn.Conv2d(1,1,kernel_size=3, bias=False) conv.weight.data = G for name, param in conv.named_parameters(): print(name, param) # take a grayscale image, and feed it into the conv model. Display the result as an image side-by-side the original. . weight Parameter containing: tensor([[0.0625, 0.1250, 0.0625], [0.1250, 0.2500, 0.1250], [0.0625, 0.1250, 0.0625]], requires_grad=True) . To illustrate the effect of other 3x3 kernels, it is best to choose the kernel close to the &quot;identity&quot;, and to apply it many times. Try the following: . A = torch.Tensor([[-1,0,1],[-2,0,2],[-1,0,1]]) B = torch.Tensor([[1,2,1],[0,0,0],[-1,-2,-1]]) # These are the Sobel operator for (traditional) edge detection I = torch.Tensor([[0,0,0],[0,1,0],[0,0,0]]) K = I + 0.01 * A conv.weight.data = K for name, param in conv.named_parameters(): print(name, param) # take a grayscale image, and feed it into the conv model. Display the result as an image side-by-side the original. . weight Parameter containing: tensor([[-0.0100, 0.0000, 0.0100], [-0.0200, 1.0000, 0.0200], [-0.0100, 0.0000, 0.0100]], requires_grad=True) . Do you see the image gets shifted in the x- or y-direction? What does it all mean? Without defining terms, one would like to say that . the operators A and B generate translations in the x- and y-directions. . That is to say, once a neural network has learned to do a certain task, we can look at each of its 3x3 kernels and say it is doing a combination of (tiny bits of) blurring, translations, and possibly other transformations. Keeping aside the activation function, with succesive convolutional layers, an image could &quot;meander&quot; through the network and shift by not-so-small a distance. By the way, this &quot;wholistic&quot; view has the advantage that one can &quot;picture&quot; the entire CNN with the same standard picture of a neural net of nodes and edges: each node is now a whole image (or feature map), and each edge is a convolution operator. [insert image] What is missing from such a convolution operator is rotation and scaling, as it is well known that CNNs are not rotation/scale-invariant, inasmuch as it is translation-invariant. There have been attempts at making a neural network invariant or equivariant for rotation and scaling, but to my limited understanding they are rather more like &quot;forced&quot; fixes than modifying the convolutional layer itself. For people with the right kinds of mathematics background, it would come immediately to mind the solution: the (infinitesimal) generators for rotation and scaling, or flow of a vector field, or solving a first-order linear partial differential equation by the method of characteristics. It appears that such people are disjoint from the Deep Learning community, or at least they have not been able to convince the Deep Learning community of their worth. . Let me first show how the conv_twist works before we get back to explain the mathematics, as simply as I could. Only then will we explain the details of the code. . conv_twist at work (no training) . In addition to a normal Conv2d layer, the conv_twist is feeding the input to two other 3x3 Conv2d layer. To see the effect, let&#39;s turn off the conv weights . model = conv_twist(1,1) model.conv.weight.data[:] = 0 model.center_x.data[:] = 0 model.center_y.data[:] = 0 model.conv_x.weight.data = I + 0.01 * A model.conv_y.weight.data = I + 0.01 * B for name, param in model.named_parameters(): print(name, param) . center_x Parameter containing: tensor([0.], requires_grad=True) center_y Parameter containing: tensor([0.], requires_grad=True) conv.weight Parameter containing: tensor([[[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]]], requires_grad=True) conv_x.weight Parameter containing: tensor([[-0.0100, 0.0000, 0.0100], [-0.0200, 1.0000, 0.0200], [-0.0100, 0.0000, 0.0100]], requires_grad=True) conv_y.weight Parameter containing: tensor([[ 0.0100, 0.0200, 0.0100], [ 0.0000, 1.0000, 0.0000], [-0.0100, -0.0200, -0.0100]], requires_grad=True) . The mathematics . There are various ways that one could discuss this pheonomenon. But the first step is invariably to think of the image not as a collection of discrete data (pixels) but as a continuous object (function of two variables), and the pixels are &quot;samples&quot; of the function that give a good approximation for computations that we want to perform on the function. . For functions of continuous variables, the first thing you might want to do is taking (partial) derivatives in the x- or y-direction. How would you approximate that if you don&#39;t have full information of the function, but only the samples at discrete points? . import numpy as np np.indices((1,4,5))[1] . array([[[0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]]]) .",
            "url": "https://liuyao12.github.io/blog/2020/03/08/2020-03-07-Conv-Twist.html",
            "relUrl": "/2020/03/08/2020-03-07-Conv-Twist.html",
            "date": " • Mar 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides has for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Feautures . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://liuyao12.github.io/blog/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://liuyao12.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}