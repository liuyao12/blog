{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Convolutional Layer with a Twist\n",
    "\n",
    "I'm exicted to introduce `conv_twist`, a replacement of (and an improvement on) the good old **convolutional layer** widely used in Deep Learning models (rightfully called ConvNets or CNN) in Computer Vision. Famously introduced by Yann LeCun some 30 years ago into image classification, it became the source of the Deep Learning/Artificial Intelligence revolution with AlexNet in 2012. Rapid improvements on the architecture followed, most importantly the ResNet of 2015. Recently attention has somewhat shifted away from image classification, but convolutional layers are still the bread and butter of any Computer Vision models. What more can be said about convolutional layers, one might ask? The answer is a little bit of mathematics.\n",
    "\n",
    "Long story short, here is the `conv_twist` in PyTorch, and you can easily swap out the 3x3 `Conv2d` in your model and plug this in, and train from scratch to see if it gives any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class conv_twist(nn.Module):  # replacing 3x3 Conv2d\n",
    "    def __init__(self, ni, nf, init_max=1.5, stride=1):\n",
    "        super(conv_twist, self).__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv_x = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv_y = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2  # make conv_x a \"first-order operator\" by symmetrizing it\n",
    "        self.conv_y.weight.data = self.conv_x.weight.transpose(2,3).flip(3)                      # make conv_y a 90 degree rotation of convx\n",
    "        self.center_x = nn.Parameter(torch.Tensor(nf), requires_grad=True)\n",
    "        self.center_y = nn.Parameter(torch.Tensor(nf), requires_grad=True)\n",
    "        self.center_x.data.uniform_(-init_max, init_max)\n",
    "        self.center_y.data.uniform_(-init_max, init_max)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2  \n",
    "        self.conv_y.weight.data = (self.conv_y.weight - self.conv_y.weight.flip(2).flip(3)) / 2\n",
    "        x1 = self.conv(x)\n",
    "        _, c, h, w = x1.size()\n",
    "        XX = torch.from_numpy(np.indices((1,h,w))[2]*2/w).type(x.dtype).to(x.device) - self.center_x.view(-1,1,1)\n",
    "        YY = torch.from_numpy(np.indices((1,h,w))[1]*2/h).type(x.dtype).to(x.device) - self.center_y.view(-1,1,1)\n",
    "        return x1 + (XX * self.conv_x(x) + YY * self.conv_y(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the weights in such a `conv_twist` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_x Parameter containing:\n",
      "tensor([ 0.3023, -0.3360,  0.5309,  0.4433, -1.0316], requires_grad=True)\n",
      "center_y Parameter containing:\n",
      "tensor([-1.1532,  0.8339,  1.0706, -1.0671,  0.9563], requires_grad=True)\n",
      "conv.weight Parameter containing:\n",
      "tensor([[[[ 0.0571,  0.0091,  0.1836],\n",
      "          [-0.1347,  0.1280,  0.0614],\n",
      "          [-0.1534,  0.0817, -0.0577]],\n",
      "\n",
      "         [[ 0.1923,  0.1592,  0.0034],\n",
      "          [ 0.1896, -0.1555, -0.1734],\n",
      "          [-0.0581, -0.1072, -0.1130]],\n",
      "\n",
      "         [[ 0.1653,  0.0071,  0.1774],\n",
      "          [-0.1583,  0.1580,  0.0131],\n",
      "          [-0.1118, -0.0300, -0.1756]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1088, -0.1590, -0.0011],\n",
      "          [-0.1373,  0.1578,  0.1774],\n",
      "          [ 0.1487, -0.0590, -0.0774]],\n",
      "\n",
      "         [[-0.0160, -0.0466,  0.1536],\n",
      "          [ 0.1157,  0.0899, -0.0484],\n",
      "          [-0.0265,  0.0195,  0.1479]],\n",
      "\n",
      "         [[ 0.1583,  0.1361, -0.1898],\n",
      "          [ 0.0298,  0.1011, -0.0624],\n",
      "          [-0.0675, -0.0696, -0.0140]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0903,  0.0801,  0.0272],\n",
      "          [-0.0263,  0.1851, -0.0386],\n",
      "          [ 0.0911,  0.0732, -0.1034]],\n",
      "\n",
      "         [[-0.1535,  0.0369,  0.1282],\n",
      "          [-0.1705, -0.1889,  0.1057],\n",
      "          [ 0.1199,  0.1284, -0.0512]],\n",
      "\n",
      "         [[-0.1427, -0.0173,  0.1785],\n",
      "          [ 0.0082,  0.0340, -0.1824],\n",
      "          [-0.1898, -0.0470, -0.1436]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1071,  0.1408,  0.1279],\n",
      "          [ 0.1466, -0.0027, -0.0033],\n",
      "          [ 0.1643,  0.1307,  0.1014]],\n",
      "\n",
      "         [[-0.1919, -0.1899,  0.0276],\n",
      "          [ 0.0095,  0.0569, -0.1876],\n",
      "          [-0.0194, -0.1519,  0.0332]],\n",
      "\n",
      "         [[-0.1761,  0.1029,  0.1825],\n",
      "          [-0.1683,  0.1804,  0.0394],\n",
      "          [ 0.0053,  0.0803,  0.1801]]],\n",
      "\n",
      "\n",
      "        [[[-0.1021, -0.0830, -0.1734],\n",
      "          [ 0.1628, -0.1476,  0.1153],\n",
      "          [ 0.0586,  0.0996, -0.1892]],\n",
      "\n",
      "         [[-0.0584, -0.1846,  0.0149],\n",
      "          [ 0.1456,  0.1168, -0.0120],\n",
      "          [ 0.1599,  0.1344,  0.0357]],\n",
      "\n",
      "         [[ 0.1706,  0.1914, -0.0694],\n",
      "          [ 0.0860,  0.0887, -0.1463],\n",
      "          [-0.0224, -0.1481,  0.0727]]]], requires_grad=True)\n",
      "conv_x.weight Parameter containing:\n",
      "tensor([[[[-7.5224e-02, -1.8295e-02,  8.3235e-02],\n",
      "          [-6.9799e-02,  0.0000e+00,  6.9799e-02],\n",
      "          [-8.3235e-02,  1.8295e-02,  7.5224e-02]],\n",
      "\n",
      "         [[-2.8982e-02,  9.3284e-02,  1.1157e-02],\n",
      "          [-1.2749e-01,  0.0000e+00,  1.2749e-01],\n",
      "          [-1.1157e-02, -9.3284e-02,  2.8982e-02]],\n",
      "\n",
      "         [[-3.8338e-02, -1.2740e-02,  7.4155e-03],\n",
      "          [-3.2470e-02,  0.0000e+00,  3.2470e-02],\n",
      "          [-7.4155e-03,  1.2740e-02,  3.8338e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.2747e-01, -3.8912e-02,  2.9849e-02],\n",
      "          [-2.3069e-02,  0.0000e+00,  2.3069e-02],\n",
      "          [-2.9849e-02,  3.8912e-02,  1.2747e-01]],\n",
      "\n",
      "         [[ 1.0544e-01, -1.1224e-01,  1.3429e-01],\n",
      "          [-2.9504e-02,  0.0000e+00,  2.9504e-02],\n",
      "          [-1.3429e-01,  1.1224e-01, -1.0544e-01]],\n",
      "\n",
      "         [[-1.2774e-01, -1.5686e-04, -6.4486e-02],\n",
      "          [ 1.1217e-01,  0.0000e+00, -1.1217e-01],\n",
      "          [ 6.4486e-02,  1.5686e-04,  1.2774e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.2844e-02, -6.1838e-02,  1.3633e-01],\n",
      "          [-4.4475e-02,  0.0000e+00,  4.4475e-02],\n",
      "          [-1.3633e-01,  6.1838e-02,  2.2844e-02]],\n",
      "\n",
      "         [[-9.4597e-03, -1.4843e-02, -5.2345e-02],\n",
      "          [-9.7515e-02,  0.0000e+00,  9.7515e-02],\n",
      "          [ 5.2345e-02,  1.4843e-02,  9.4597e-03]],\n",
      "\n",
      "         [[ 9.6781e-02, -7.2473e-02, -4.2748e-02],\n",
      "          [-4.4189e-02,  0.0000e+00,  4.4189e-02],\n",
      "          [ 4.2748e-02,  7.2473e-02, -9.6781e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.2418e-02, -1.1462e-01,  1.1140e-01],\n",
      "          [-5.5214e-02,  0.0000e+00,  5.5214e-02],\n",
      "          [-1.1140e-01,  1.1462e-01, -5.2418e-02]],\n",
      "\n",
      "         [[ 1.6125e-01,  6.7214e-02, -1.5112e-02],\n",
      "          [-2.8123e-02,  0.0000e+00,  2.8123e-02],\n",
      "          [ 1.5112e-02, -6.7214e-02, -1.6125e-01]],\n",
      "\n",
      "         [[ 3.7767e-02, -5.2222e-02,  1.2324e-01],\n",
      "          [ 5.3436e-02,  0.0000e+00, -5.3436e-02],\n",
      "          [-1.2324e-01,  5.2222e-02, -3.7767e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3408e-01, -2.7979e-02,  3.5268e-02],\n",
      "          [ 1.1753e-01,  0.0000e+00, -1.1753e-01],\n",
      "          [-3.5268e-02,  2.7979e-02, -1.3408e-01]],\n",
      "\n",
      "         [[-1.2700e-01,  7.2318e-02,  4.9036e-03],\n",
      "          [ 2.9414e-02,  0.0000e+00, -2.9414e-02],\n",
      "          [-4.9036e-03, -7.2318e-02,  1.2700e-01]],\n",
      "\n",
      "         [[ 1.9118e-02,  5.3294e-02, -3.2968e-02],\n",
      "          [ 3.5216e-02,  0.0000e+00, -3.5216e-02],\n",
      "          [ 3.2968e-02, -5.3294e-02, -1.9118e-02]]]], requires_grad=True)\n",
      "conv_y.weight Parameter containing:\n",
      "tensor([[[[-8.3235e-02, -6.9799e-02, -7.5224e-02],\n",
      "          [ 1.8295e-02,  0.0000e+00, -1.8295e-02],\n",
      "          [ 7.5224e-02,  6.9799e-02,  8.3235e-02]],\n",
      "\n",
      "         [[-1.1157e-02, -1.2749e-01, -2.8982e-02],\n",
      "          [-9.3284e-02,  0.0000e+00,  9.3284e-02],\n",
      "          [ 2.8982e-02,  1.2749e-01,  1.1157e-02]],\n",
      "\n",
      "         [[-7.4155e-03, -3.2470e-02, -3.8338e-02],\n",
      "          [ 1.2740e-02,  0.0000e+00, -1.2740e-02],\n",
      "          [ 3.8338e-02,  3.2470e-02,  7.4155e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.9849e-02, -2.3069e-02, -1.2747e-01],\n",
      "          [ 3.8912e-02,  0.0000e+00, -3.8912e-02],\n",
      "          [ 1.2747e-01,  2.3069e-02,  2.9849e-02]],\n",
      "\n",
      "         [[-1.3429e-01, -2.9504e-02,  1.0544e-01],\n",
      "          [ 1.1224e-01,  0.0000e+00, -1.1224e-01],\n",
      "          [-1.0544e-01,  2.9504e-02,  1.3429e-01]],\n",
      "\n",
      "         [[ 6.4486e-02,  1.1217e-01, -1.2774e-01],\n",
      "          [ 1.5686e-04,  0.0000e+00, -1.5686e-04],\n",
      "          [ 1.2774e-01, -1.1217e-01, -6.4486e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.3633e-01, -4.4475e-02, -2.2844e-02],\n",
      "          [ 6.1838e-02,  0.0000e+00, -6.1838e-02],\n",
      "          [ 2.2844e-02,  4.4475e-02,  1.3633e-01]],\n",
      "\n",
      "         [[ 5.2345e-02, -9.7515e-02, -9.4597e-03],\n",
      "          [ 1.4843e-02,  0.0000e+00, -1.4843e-02],\n",
      "          [ 9.4597e-03,  9.7515e-02, -5.2345e-02]],\n",
      "\n",
      "         [[ 4.2748e-02, -4.4189e-02,  9.6781e-02],\n",
      "          [ 7.2473e-02,  0.0000e+00, -7.2473e-02],\n",
      "          [-9.6781e-02,  4.4189e-02, -4.2748e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.1140e-01, -5.5214e-02,  5.2418e-02],\n",
      "          [ 1.1462e-01,  0.0000e+00, -1.1462e-01],\n",
      "          [-5.2418e-02,  5.5214e-02,  1.1140e-01]],\n",
      "\n",
      "         [[ 1.5112e-02, -2.8123e-02,  1.6125e-01],\n",
      "          [-6.7214e-02,  0.0000e+00,  6.7214e-02],\n",
      "          [-1.6125e-01,  2.8123e-02, -1.5112e-02]],\n",
      "\n",
      "         [[-1.2324e-01,  5.3436e-02,  3.7767e-02],\n",
      "          [ 5.2222e-02,  0.0000e+00, -5.2222e-02],\n",
      "          [-3.7767e-02, -5.3436e-02,  1.2324e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.5268e-02,  1.1753e-01,  1.3408e-01],\n",
      "          [ 2.7979e-02,  0.0000e+00, -2.7979e-02],\n",
      "          [-1.3408e-01, -1.1753e-01,  3.5268e-02]],\n",
      "\n",
      "         [[-4.9036e-03,  2.9414e-02, -1.2700e-01],\n",
      "          [-7.2318e-02,  0.0000e+00,  7.2318e-02],\n",
      "          [ 1.2700e-01, -2.9414e-02,  4.9036e-03]],\n",
      "\n",
      "         [[ 3.2968e-02,  3.5216e-02,  1.9118e-02],\n",
      "          [-5.3294e-02,  0.0000e+00,  5.3294e-02],\n",
      "          [-1.9118e-02, -3.5216e-02, -3.2968e-02]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = conv_twist(3,5)\n",
    "for name, para in model.named_parameters():\n",
    "    print(name, para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at the `conv_x` weights, you'd notice that each 3x3 kernel is symmetric (the numbers on the opposite ends are identical except for the signs, with the middle one always 0). That's what I mean by \"symmetrizing\" `conv_x`. You can also check that the `conv_x` and `conv_y` weights are identical but off by a 90 degree rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0752, -0.0183,  0.0832],\n",
       "         [-0.0698,  0.0000,  0.0698],\n",
       "         [-0.0832,  0.0183,  0.0752]], grad_fn=<SelectBackward>),\n",
       " tensor([[-0.0832, -0.0698, -0.0752],\n",
       "         [ 0.0183,  0.0000, -0.0183],\n",
       "         [ 0.0752,  0.0698,  0.0832]], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv_x.weight[0,0], model.conv_y.weight[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do I choose to initialize the weights this way? Well, I'll try to explain it later. But for now it's important to note that `conv_twist` is a lot bigger than the standard `Conv2d` layer, but not as much as it appears to be. This particular implementation, if I had done it properly, has about twice as many trainable weights as a single `Conv2d` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are convolutions?\n",
    "\n",
    "The standard story in Neural Networks is that the convolution operator captures the spatial relation of the pixels, so is particularly suited for image-related learning task, and has much fewer weights, than a generic linear map (fully connected layer). And the consensus has come down to using only 3x3 convolutions, and go deeper (i.e., many layers), hence *deep* learning.\n",
    "\n",
    "What is perhaps not well-known is that different 3x3 kernels have very intuitive meanings, in terms of what it does to the image. For example, the Gaussian kernel in image processing \"blurs\" the image. We can do a little experiment to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
