{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Convolutional Layer with a Twist\n",
    "\n",
    "I'm excited to introduce `conv_twist`, a replacement of (and an improvement on) the good old **convolutional layer** widely used in Deep Learning models (rightfully referred to as ConvNets or CNN) in Computer Vision. Famously introduced by Yann LeCun some 30 years ago into image classification, it became the source of the Deep Learning/Artificial Intelligence revolution with AlexNet in 2012. Rapid improvements on the architecture followed, most notably the ResNet of 2015. Recently attention has somewhat shifted away from image classification, but convolutional layers are still the bread and butter of any Computer Vision models. What more can be said about convolutional layers, one might ask? The answer comes from a little bit of mathematics.\n",
    "\n",
    "Long story short, here is one implementation of `conv_twist` in PyTorch, and you can easily swap out the 3x3 `Conv2d` in your model and plug this in, and train from scratch to see if it gives any improvement. (Report on results are welcome.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class conv_twist(nn.Module):  # replacing 3x3 Conv2d\n",
    "    def __init__(self, ni, nf, init_max=1.5, stride=1):\n",
    "        super(conv_twist, self).__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv_x = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv_y = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2  # make conv_x a \"first-order operator\" by symmetrizing it\n",
    "        self.conv_y.weight.data = self.conv_x.weight.transpose(2,3).flip(3)                      # make conv_y a 90 degree rotation of convx\n",
    "        self.center_x = nn.Parameter(torch.Tensor(nf), requires_grad=True)\n",
    "        self.center_y = nn.Parameter(torch.Tensor(nf), requires_grad=True)\n",
    "        self.center_x.data.uniform_(-init_max, init_max)\n",
    "        self.center_y.data.uniform_(-init_max, init_max)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2  \n",
    "        self.conv_y.weight.data = (self.conv_y.weight - self.conv_y.weight.flip(2).flip(3)) / 2\n",
    "        x1 = self.conv(x)\n",
    "        _, c, h, w = x1.size()\n",
    "        XX = torch.from_numpy(np.indices((1,h,w))[2]*2/w).type(x.dtype).to(x.device) - self.center_x.view(-1,1,1)\n",
    "        YY = torch.from_numpy(np.indices((1,h,w))[1]*2/h).type(x.dtype).to(x.device) - self.center_y.view(-1,1,1)\n",
    "        return x1 + (XX * self.conv_x(x) + YY * self.conv_y(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the weights in such a `conv_twist` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_x Parameter containing:\n",
      "tensor([0.2157], requires_grad=True)\n",
      "center_y Parameter containing:\n",
      "tensor([0.9144], requires_grad=True)\n",
      "conv.weight Parameter containing:\n",
      "tensor([[[[ 0.0164,  0.0767, -0.2709],\n",
      "          [-0.3086, -0.0439,  0.1440],\n",
      "          [-0.2906,  0.0374,  0.2853]]]], requires_grad=True)\n",
      "conv_x.weight Parameter containing:\n",
      "tensor([[[[ 0.2468, -0.2211, -0.2301],\n",
      "          [-0.0661,  0.0000,  0.0661],\n",
      "          [ 0.2301,  0.2211, -0.2468]]]], requires_grad=True)\n",
      "conv_y.weight Parameter containing:\n",
      "tensor([[[[ 0.2301, -0.0661,  0.2468],\n",
      "          [ 0.2211,  0.0000, -0.2211],\n",
      "          [-0.2468,  0.0661, -0.2301]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = conv_twist(1,1)\n",
    "for name, para in model.named_parameters():\n",
    "    print(name, para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at the `conv_x` weights, you'd notice that each 3x3 kernel is symmetric (the numbers on the opposite ends are identical except for the signs, with the middle one always 0). That's the effect of \"symmetrizing\" on `conv_x`, done at each forward pass. You can also check that the `conv_x` and `conv_y` weights are initialized to be identical but off by a 90 degree rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do I choose to initialize the weights this way? Well, I'll try to explain all this later. For now it's important to note that `conv_twist` is a lot bigger than the standard `Conv2d` layer, but not as much as it appears to be. This particular implementation, if I had done it properly, has about twice as many trainable weights as a single `Conv2d` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are convolutions?\n",
    "\n",
    "The classic storyline in Neural Networks is that the convolution operator captures the spatial relation of the pixels, so is particularly suited for image-related learning task, and has much fewer weights than a generic linear map (fully connected layer). And over the years people learned that we don't need kernels that are larger than 3x3, and to go deeper (i.e., many layers) instead, hence *deep* learning.\n",
    "\n",
    "What is perhaps not well-known is that different 3x3 kernels have rather intuitive meanings, in terms of what it does to the image *overall*. For example, the Gaussian kernel in image processing \"blurs\" the image. We can do a little experiment to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight Parameter containing:\n",
      "tensor([[0.0625, 0.1250, 0.0625],\n",
      "        [0.1250, 0.2500, 0.1250],\n",
      "        [0.0625, 0.1250, 0.0625]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "G = torch.Tensor([[1,2,1],[2,4,2],[1,2,1]])/16\n",
    "conv = nn.Conv2d(1,1,kernel_size=3, bias=False)\n",
    "conv.weight.data = G\n",
    "for name, param in conv.named_parameters():\n",
    "    print(name, param)\n",
    "    \n",
    "# take a grayscale image, and feed it into the conv model. Display the result as an image side-by-side the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the effect for other 3x3 kernels, it seems best to choose the kernel close to the \"identity\", and to apply it many times. Try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "B = torch.Tensor([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "I = torch.Tensor([[0,0,0],[0,1,0],[0,0,0]])\n",
    "K = I + 0.01 * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does `conv_twist` do?\n",
    "\n",
    "In addition to a normal Conv2d layer, the `conv_twist` is feeding the input to two other 3x3 Conv2d layer. To see the effect, let's turn off the `conv` weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv.data = 0\n",
    "model.center_x.data = torch.Tensor([0])\n",
    "model.center_y.data = torch.Tensor([0])\n",
    "model.conv_x.data = I + 0.01 * A\n",
    "model.conv_y.data = I + 0.01 * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
