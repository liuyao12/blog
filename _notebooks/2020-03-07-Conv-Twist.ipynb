{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layer with a Twist\n",
    "> Modification of Conv2d that has a \"built-in\" capacity of learning to rotate/dilate the image. Easy plug-in for any CNN model.\n",
    "\n",
    "- toc: true\n",
    "- author: Liu Yao 刘杳\n",
    "- comments: true\n",
    "- categories: [research]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing...\n",
    "\n",
    "I'm excited to introduce `conv_twist`, a replacement of—and, I argue, an improvement on—the good old **convolutional layer** widely used in Deep Learning models (rightfully referred to as ConvNets or CNNs) in Computer Vision. Famously introduced by Yann LeCun some 30 years ago into image classification, it became the source of the Deep Learning/Artificial Intelligence revolution with AlexNet in 2012. Rapid improvements on the architecture followed, most notably the ResNet of 2015. Recently attention has somewhat shifted away from image classification, but convolutional layers are still the bread and butter of any Computer Vision models. What more can be said about convolutional layers, one might ask? The answer is a little bit of mathematics.\n",
    "\n",
    "For what it's worth, this is not peer-reviewed or published in any conference. If you'd like to give it a \"review\" on Twitter, feel free to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without further ado, here is one (crude) implementation of `conv_twist` in PyTorch, and you can easily swap out the 3x3 `Conv2d` in your model and plug this in, and train *from scratch* to see if it gives any improvement. Early success has been shown on [Imagenette/Imagewoof](https://github.com/fastai/imagenette) benchmark. (Help with testing on other models/datasets are greatly appreciated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "PIL.PILLOW_VERSION = PIL.__version__\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class conv_twist(nn.Module):  # replacing 3x3 Conv2d\n",
    "    def __init__(self, ni, nf, init_max=1.5, stride=1):\n",
    "        super(conv_twist, self).__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv_x = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv_y = nn.Conv2d(ni, nf, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2  # make conv_x a \"first-order operator\" by symmetrizing it\n",
    "        self.conv_y.weight.data = self.conv_x.weight.transpose(2,3).flip(3)                      # make conv_y a 90 degree rotation of conv_x\n",
    "        self.center_x = nn.Parameter(torch.Tensor(nf), requires_grad=True)\n",
    "        self.center_y = nn.Parameter(torch.Tensor(nf), requires_grad=True)\n",
    "        self.center_x.data.uniform_(-init_max, init_max)\n",
    "        self.center_y.data.uniform_(-init_max, init_max)\n",
    "\n",
    "    def forward(self, inpt):\n",
    "        self.conv_x.weight.data = (self.conv_x.weight - self.conv_x.weight.flip(2).flip(3)) / 2  \n",
    "        self.conv_y.weight.data = (self.conv_y.weight - self.conv_y.weight.flip(2).flip(3)) / 2\n",
    "        out = self.conv(inpt)\n",
    "        _, c, h, w = out.size()\n",
    "        XX = torch.from_numpy(np.indices((1,h,w))[2]*2/w).type(out.dtype).to(out.device) - self.center_x.view(-1,1,1)\n",
    "        YY = torch.from_numpy(np.indices((1,h,w))[1]*2/h).type(out.dtype).to(out.device) - self.center_y.view(-1,1,1)\n",
    "        return out + (XX * self.conv_x(inpt) + YY * self.conv_y(inpt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the (initial) weights in such a `conv_twist` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_x Parameter containing:\n",
      "tensor([0.5226], requires_grad=True)\n",
      "center_y Parameter containing:\n",
      "tensor([0.5464], requires_grad=True)\n",
      "conv.weight Parameter containing:\n",
      "tensor([[[[-0.0298, -0.0940,  0.1901],\n",
      "          [-0.1259,  0.1577,  0.0541],\n",
      "          [ 0.0786, -0.0098, -0.0743]],\n",
      "\n",
      "         [[ 0.0559,  0.0047,  0.0455],\n",
      "          [ 0.0163,  0.1859,  0.0017],\n",
      "          [ 0.1650, -0.1146, -0.0215]],\n",
      "\n",
      "         [[ 0.0070,  0.0852,  0.0348],\n",
      "          [ 0.1815,  0.0406,  0.0865],\n",
      "          [ 0.1346, -0.0909,  0.1083]]]], requires_grad=True)\n",
      "conv_x.weight Parameter containing:\n",
      "tensor([[[[ 0.0591,  0.0362,  0.1139],\n",
      "          [-0.1468,  0.0000,  0.1468],\n",
      "          [-0.1139, -0.0362, -0.0591]],\n",
      "\n",
      "         [[ 0.0501, -0.0121, -0.0393],\n",
      "          [-0.0586,  0.0000,  0.0586],\n",
      "          [ 0.0393,  0.0121, -0.0501]],\n",
      "\n",
      "         [[ 0.1027,  0.1131,  0.0566],\n",
      "          [ 0.1376,  0.0000, -0.1376],\n",
      "          [-0.0566, -0.1131, -0.1027]]]], requires_grad=True)\n",
      "conv_y.weight Parameter containing:\n",
      "tensor([[[[-0.1139, -0.1468,  0.0591],\n",
      "          [-0.0362,  0.0000,  0.0362],\n",
      "          [-0.0591,  0.1468,  0.1139]],\n",
      "\n",
      "         [[ 0.0393, -0.0586,  0.0501],\n",
      "          [ 0.0121,  0.0000, -0.0121],\n",
      "          [-0.0501,  0.0586, -0.0393]],\n",
      "\n",
      "         [[-0.0566,  0.1376,  0.1027],\n",
      "          [-0.1131,  0.0000,  0.1131],\n",
      "          [-0.1027, -0.1376,  0.0566]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = conv_twist(3,1)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at the `conv_x` and `conv_y` weights, you'd notice that each 3x3 kernel is symmetric (the numbers on the opposite ends of the square are identical but with opposite signs, with the middle one always 0). That's the effect of \"symmetrizing\", done at each forward pass. You can also check that `conv_y` weights are initialized to be a 90 degree rotation of `conv_x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do I choose to initialize the weights this way, and what are `XX` and `YY`? Well, I'll try to explain everything later. For now it's important to note that `conv_twist` is a lot bigger than the standard `Conv2d` layer, but not as much as it appears to be. This particular implementation, if I had done it properly, has about twice as many trainable weights as a simple `Conv2d` layer.\n",
    "\n",
    "Let's first revisit convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do convolutions actually do?\n",
    "\n",
    "It is by now common knowledge that the convolution operator captures the spatial relation of the pixels (local features), so is particularly suited for image-related learning task. Moreover, it uses much fewer weights than a generic linear map (fully connected layer). The classic storyline goes like this: the first layer would learn about the line pattern, the next one how those lines come together; the deeper you go, the \"higher-level structure\" the convolutions learn to capture. It is also common knowledge that this is just heuristics, and should not be taken too literally. In addition, over the years we have learned that it's better to do away with kernels that are larger than 3x3 (and do away with any biases), and to go deeper (insert more layers) instead.\n",
    "\n",
    "What is perhaps not well-known is that different 3x3 kernels have rather intuitive meanings, in terms of what it does to the image *overall*. For example, the Gaussian kernel in image processing \"blurs\" the image. We can do a little experiment to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight Parameter containing:\n",
      "tensor([[0.0625, 0.1250, 0.0625],\n",
      "        [0.1250, 0.2500, 0.1250],\n",
      "        [0.0625, 0.1250, 0.0625]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "G = torch.Tensor([[1,2,1],[2,4,2],[1,2,1]]) / 16\n",
    "conv = nn.Conv2d(1,1,kernel_size=3,bias=False)\n",
    "conv.weight.data = G\n",
    "for name, param in conv.named_parameters():\n",
    "    print(name, param)\n",
    "    \n",
    "# take a grayscale image, and feed it into the conv model. Display the result as an image side-by-side the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the effect of other 3x3 kernels, it is best to choose the kernel close to the \"identity\", and to apply it many times. Try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight Parameter containing:\n",
      "tensor([[-0.0100,  0.0000,  0.0100],\n",
      "        [-0.0200,  1.0000,  0.0200],\n",
      "        [-0.0100,  0.0000,  0.0100]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "B = torch.Tensor([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "# These are the Sobel operator for (traditional) edge detection\n",
    "I = torch.Tensor([[0,0,0],[0,1,0],[0,0,0]])\n",
    "K = I + 0.01 * A  # or B\n",
    "conv.weight.data = K\n",
    "for name, param in conv.named_parameters():\n",
    "    print(name, param)\n",
    "    \n",
    "# take a grayscale image, and feed it into the conv model. Display the result as an image side-by-side the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the image gets shifted in the *x*- or *y*-direction? What does it all mean? Without defining terms, one would like to say that \n",
    "> the operators `A` and `B` **generate** translations in the *x*- and *y*-directions.\n",
    "\n",
    "You can also check for yourself that `A+B` generates translation in the 45 degree direction. Similarly, you can \"mix up\" translation with blurring. In fact, *any* 3x3 convolution is a combination of (tiny bits of) blurring, translations, and possibly other transformations. That's what convolution *does* to the whole image. By the way, this \"wholistic\" point of view has the advantage that one can now \"picture\" the entire CNN with the same standard picture of a neural net of nodes and edges: each node is now a whole image (or feature map), and each edge is a convolution operator. (insert image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is missing from such a convolutional operator is **rotation** and **scaling**, as it is well known that CNNs are not rotation/scale-invariant, inasmuch as it is translation-invariant. There have been attempts at making a neural network invariant or *equivariant* for rotation and scaling, but to my limited understanding they are rather more like \"forced\" fixes than modifying the convolutional layer itself. For people with the right kind of mathematical backgrounds, the solution may come immediately to mind: the **(infinitesimal) generators** for rotation and scaling (**Lie algebra** of a **Lie group**), or **flow** of a **vector field**, or solving a first-order linear partial differential equation by the **method of characteristics**.\n",
    "\n",
    "Let me first show how the `conv_twist` works before we get back to explain the mathematics, as simply as I could (not using any of the jargon words above). Only then will we explain the details of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `conv_twist` at work (no training)\n",
    "\n",
    "In addition to a standard Conv2d layer, the `conv_twist` is feeding the input to two other 3x3 Conv2d layer. To see the effect, let's turn off the `conv` weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_x Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "center_y Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "conv.weight Parameter containing:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 0.]], requires_grad=True)\n",
      "conv_x.weight Parameter containing:\n",
      "tensor([[-0.0100,  0.0000,  0.0100],\n",
      "        [-0.0200,  0.0000,  0.0200],\n",
      "        [-0.0100,  0.0000,  0.0100]], requires_grad=True)\n",
      "conv_y.weight Parameter containing:\n",
      "tensor([[ 0.0100,  0.0200,  0.0100],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0100, -0.0200, -0.0100]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = conv_twist(1,1)\n",
    "model.conv.weight.data = I\n",
    "model.center_x.data[:] = 0\n",
    "model.center_y.data[:] = 0\n",
    "model.conv_x.weight.data = 0.01 * A  # or -B\n",
    "model.conv_y.weight.data = 0.01 * B  # or A\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param)\n",
    "    \n",
    "# take a grayscale image, and feed it into the conv model. Display the result as an image side-by-side the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The math behind all this\n",
    "\n",
    "There are various ways that one could discuss the pheonomenon of a \"local operator\", when applied many times, yields a global transformation. But the first step is invariably to think of the image *not* as a collection of **discrete** data (pixels) but as a **continuous** object (function of two variables), and the pixels are \"samples\" of the function that give a good approximation for computations that we want to perform on the function.\n",
    "\n",
    "For functions of continuous variables, the first thing you might want to do is taking (partial) derivatives in the x- or y-direction. How would you approximate that if you don't have full information of the function, but only the samples at discrete points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 0 0 0 0 0]\n",
      "  [1 1 1 1 1 1 1 1]\n",
      "  [2 2 2 2 2 2 2 2]\n",
      "  [3 3 3 3 3 3 3 3]\n",
      "  [4 4 4 4 4 4 4 4]\n",
      "  [5 5 5 5 5 5 5 5]]]\n",
      "[[[0 1 2 3 4 5 6 7]\n",
      "  [0 1 2 3 4 5 6 7]\n",
      "  [0 1 2 3 4 5 6 7]\n",
      "  [0 1 2 3 4 5 6 7]\n",
      "  [0 1 2 3 4 5 6 7]\n",
      "  [0 1 2 3 4 5 6 7]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.indices((1,6,8))[1])\n",
    "print(np.indices((1,6,8))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
